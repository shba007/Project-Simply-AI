{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import deeplake\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"D:/Programming/Projects/Public/plant-lens/ai\"\n",
    "DATASET_DATA_PATH = f\"{ROOT_PATH}/data/dataset\"\n",
    "\n",
    "training_dataset = deeplake.load(f'{DATASET_DATA_PATH}/training')\n",
    "testing_dataset = deeplake.load(f'{DATASET_DATA_PATH}/testing')\n",
    "\n",
    "training_dataset.summary()\n",
    "testing_dataset.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes = list(set(sum(training_dataset.labels.data()['text'], [])))\n",
    "data_classes_count = len(data_classes)\n",
    "print(\"Total Number of Classes\", data_classes_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(training_dataset.labels) * 0.8)\n",
    "print(\"Train Data Size\", training_size, \"Validation Data Size\", len(training_dataset.labels) - training_size)\n",
    "\n",
    "training_dataset = training_dataset.tensorflow()\n",
    "testing_dataset = testing_dataset.tensorflow()\n",
    "\n",
    "validation_dataset = training_dataset.skip(training_size)\n",
    "training_dataset = training_dataset.take(training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function\n",
    "def preprocess_image(args)->tuple:\n",
    "  image = args['images']\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = tf.math.divide(image, 255.0)\n",
    "\n",
    "  label = args['labels']\n",
    "  label = tf.cast(label, tf.int32)\n",
    "  label = tf.squeeze(label)\n",
    "\n",
    "  encoded_label = tf.one_hot(label, depth=data_classes_count)\n",
    "  return (image, encoded_label)\n",
    "\n",
    "training_dataset = training_dataset.map(preprocess_image)\n",
    "validation_dataset = validation_dataset.map(preprocess_image)\n",
    "testing_dataset = testing_dataset.map(preprocess_image)\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "BATCH_SIZE = 32\n",
    "training_dataset = training_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "testing_dataset = testing_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in training_dataset.take(1):\n",
    "  print(\"Training Batch images shape:\", images.shape)\n",
    "  print(\"Training Batch labels shape:\", labels.shape)\n",
    "\n",
    "for images, labels in testing_dataset.take(1):\n",
    "  print(\"Testing Batch images shape:\", images.shape)\n",
    "  print(\"Testing Batch labels shape:\", labels.shape)\n",
    "\n",
    "for images, labels in validation_dataset.take(1):\n",
    "  print(\"Validation Batch images shape:\", images.shape)\n",
    "  print(\"Validation Batch labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, layers, optimizers, models, callbacks\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    dimensions = 224\n",
    "    input_layer = layers.Input(shape=(dimensions, dimensions, 3))\n",
    "    feature_extractor = hub.KerasLayer('https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5', trainable=True)(input_layer)\n",
    "    flatten_layer = layers.Flatten()(feature_extractor)\n",
    "    hidden_layer = layers.Dense(512, activation='relu')(flatten_layer)\n",
    "    predictions = layers.Dense(data_classes_count, activation='softmax')(hidden_layer)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "    LEARNING_RATE = 0.0001\n",
    "    optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "   \n",
    "    return model\n",
    "\n",
    "DEVELOPMENT_MODEL_PATH = f\"{ROOT_PATH}/model/develop\"\n",
    "VERSION_TAG = \"0.2.0-46\"\n",
    "BUILD_MODEL = True\n",
    "\n",
    "if BUILD_MODEL:\n",
    "  model = build_model()\n",
    "else:\n",
    "  model = models.load_model(f'{DEVELOPMENT_MODEL_PATH}/v{VERSION_TAG}.keras')\n",
    "  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 50\n",
    "CHECKPOINT_MODEL_PATH = f\"{ROOT_PATH}/model/checkpoint/VERSION\"\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(filepath=CHECKPOINT_MODEL_PATH, save_weights_only=True, verbose=1)\n",
    "# Train the model\n",
    "history = model.fit(training_dataset, epochs=EPOCHS, validation_data=validation_dataset, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access metrics from training history\n",
    "print(\"\\ninitial | Training loss: {:.4f} | Validation Loss: {:.4f} || Training Accuracy: {:.2f} % | Validation Accuracy: {:.2f} %\".format(history.history[\"loss\"][0], history.history[\"val_loss\"][0], history.history[\"accuracy\"][0] * 100, history.history[\"val_accuracy\"][0] * 100))\n",
    "print(\" latest | Training loss: {:.4f} | Validation Loss: {:.4f} || Training Accuracy: {:.2f} % | Validation Accuracy: {:.2f} %\".format(history.history[\"loss\"][-1], history.history[\"val_loss\"][-1], history.history[\"accuracy\"][-1] * 100, history.history[\"val_accuracy\"][-1] * 100))\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, figsize=(10,4)) \n",
    "# plot loss\n",
    "axis[0].plot(history.history[\"loss\"])\n",
    "axis[0].plot(history.history[\"val_loss\"], color='orange')\n",
    "axis[0].set_title(\"loss per epoch\")\n",
    "# plot accuracy\n",
    "axis[1].plot(history.history[\"accuracy\"])\n",
    "axis[1].plot(history.history[\"val_accuracy\"], color='orange')\n",
    "axis[1].set_title(\"accuracy per epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(testing_dataset)\n",
    "\n",
    "print('Test loss:', test_loss, 'Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "true_labels = []\n",
    "for _, labels in testing_dataset:\n",
    "    true_labels.extend(tf.math.argmax(labels, axis=1).numpy())\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "predicted_labels = tf.math.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=data_classes)\n",
    "\n",
    "cm_display.plot()\n",
    "\n",
    "# Rotate x-axis labels by 15 degrees\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_TAG = input(\"Enter Version Tag (e.g 0.0.0):\") + f\"-{int(test_acc * 100)}\" if BUILD_MODEL else VERSION_TAG # \"0.0.0\"\n",
    "\n",
    "models.save_model(model, f'{DEVELOPMENT_MODEL_PATH}/v{VERSION_TAG}', save_format=\"tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
